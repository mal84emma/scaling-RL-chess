# Important project notes

- I've completely restructured the original repo because it had some odd features
- Some stochasticity needed to be added to Stockfish to avoid all of the simulated training games being the same. Currently it probabilistically choose a move 1 position better or worse that chosen by its elo rating.
- The learning strategy has been rethought. Now instead of trying to directly learn the policy mapping (via the policy head), the strategy is to learn a value function (approximating a Q function) which represents how good each board position is for the player about to move. The policy is then an exhaustive search of the legal moves to identify the one which provides the lowest value function (minimal goodness for opponent who will play after move taken).
The hope is that this makes training easier. Importantly, we can use Stockfish to get extremely dense rewards (i.e. score for each position in the training set), rather than relying on one reward signal per game.
- This means the Model Predictive fine-tuning strategy has also changed. The plan is, from the current position, for each move, play out the game $n$ times for $m$ steps each using the current agent, evaluate the centipawn scores of the end-points using Stockfish (simulated environment costs), and then use the average end-point scores for each move as the fine-tuning data. (Yes, this is a slightly odd thing to do, the Model Predictive information is possibly too good here - but might be more sensible if we iterate the agent a few times, i.e. address on-policy search)
- The Win-Draw-Loss score seems more sensible (centipawn scores are not a perfect representation of how good a position is), but there is an issue in the end game that the engine assigns most of the moves 0 scores for the opponent (i.e. they have definitely lost). But this misses the information about which move is actually winning for the agent, and so it can only draw in a dominant position. Basically WDL reward saturates very problematically.
Centipawn scores have their issues, mate is numerically very spikey (the score assigned to mate position is an important numerical parameter). But with perfect Centipawn scores the agent is able to outperform the best human players (c. 3000 elo).
- Starting chess games is suuuuper hard, opening theory is crazy, it might be good to start test games using an opening book to get into a reasonable position and not get absolutely destroyed in the first 10 moves. Need to see how the agent is able to perform.

- Neural Network training is a complete dark art. Loads of factors affect performance in murky and interdependent ways; the learning rate (and schedule), the batch size, the architecture (e.g. no. of hidden layers, no. of filters, no. of residual layers), and so on. How do you even start to tune these things? I have no idea. [Some people](https://github.com/dogeystamp/chess_inator) claim to get very impressive results from very small networks, whereas AlphaZero used a huge network and tonnes of compute.
- The recommended settings turn out to be really quite good for the task and difficult to improve on (shock, turns out DeepMind are smart).
- Bigger is not always better. Often when I used a larger network it got stuck during training at a local minimum with a significantly higher loss. Training very large models is difficult, and can lead to their extra power being wasted. Interestingly the loss at which the models got stuck was pretty consistent across different hyper-parameter choices.

- Current model, `Tmodel`, trained using command ```python scripts/train_model.py data/models/Tmodel data/positions/positions.json --epochs 50 --bs 256 --vs 0.1 ``` - ran a 2nd time to improve training a bit.
- The current model can occasionally get itself into good positions, but it frequently blunders them and doesn't seem to have learned checkmate. In fairness, it's not a big model and hasn't been trained on much data. It doesn't seem to have picked up a few basic tactics, e.g. progressing pawns to promotion.